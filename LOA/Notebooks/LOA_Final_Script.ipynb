{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in indexes: https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas --index-url https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in indexes: https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy --index-url https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention all file names as string in the below list which are available at \"raw_data_path\" directory\n",
    "raw_data_train = ['train_raw_data.csv']\n",
    "\n",
    "raw_data_X_test = ['test_raw_data.csv']\n",
    "\n",
    "# Mention the suffix which will used in file name to store pre-processed-data, transformed data and predition results \n",
    "fname_suffix = 'loa.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "# FOR LOADING RAW DATA\n",
    "# Mention the bucket name where raw data is present\n",
    "bucket='adl-core-sagemaker-studio'\n",
    "\n",
    "# Mention the correct directory where raw-data is present, if data is in multiple files make sure all files are present in this directory\n",
    "raw_data_path = f'external/artichauhan/LOA/loa_script/Data'\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR X DAYS LOGIC FOR CREATING TARGET VARIABLE\n",
    "# n-> number of days to look forward and check of participant is contributing in HSA or not.\n",
    "# At time this use-case was worked upon, it was 30 days, so 'n' is taken as 30\n",
    "# This value is use at code line 26\n",
    "n = 30\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR STORING PRE-PROCESSED DATA\n",
    "# This is used to seperate different versions of data for whole lifecycle i.e., data pre-process, transformations, modeling and inferences.\n",
    "#Make sure \"version\" name is as per version of data pre-processing script. Eg. dpp0-xgb-v1-final.ipynb means \"version-1\"\n",
    "version = 'version-1'\n",
    "\n",
    "# Mention the directory where pre-processed data will be stored with file name\n",
    "#pre_processed_data_path_with_fname = f'external/artichauhan/LOA/loa_script/Artifacts/{version}/preprocessed-train-data-{fname_suffix}'\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR STORING TRANSFORMED DATA\n",
    "# This is used to seperate different versions of data transformation pipelines\n",
    "# Make sure \"dppn\" name is as per version of data transformation script. Eg. dpp0-xgb-v1-final.ipynb means \"dpp0\"\n",
    "dppn = 'dpp0'\n",
    "\n",
    "# Mention the directory where pre-processed data will be stored (data that will get at end of this script)\n",
    "pre_processed_train_data_path = f'external/artichauhan/LOA/loa_script/Artifacts/{version}/preprocessed-train-data/{dppn}-train-{fname_suffix}'\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR SAVING PREPROCESSED TEST DATA\n",
    "# Mention the path where pre-processed test data will be stored in S3\n",
    "pre_processed_test_data_path = f'external/artichauhan/LOA/loa_script/Artifacts/{version}/preprocessed-test-data/{dppn}-x-test-{fname_suffix}'\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR LOADING TRAINED ML MODEL\n",
    "# Mention the directory with file name where trained ML model is stored (naming convention: version-dppn-model.pkl)\n",
    "model_path_with_fname = f'external/artichauhan/LOA/loa_script/Model/{version}/{dppn}-xgb.pkl'\n",
    "\n",
    "#####################################################################################################################\n",
    "# FOR STORING PREDICTION RESULTS\n",
    "# Mention the directory where final predction out will be stored.\n",
    "# It will save file in CSV format, 1st column contains \"platform_internal_id\" and 2nd column will have predicted label 1/0\n",
    "prediction_output_path_with_fname = f'external/artichauhan/LOA/loa_script/Output/{version}/{dppn}-xgb-out-{fname_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing librarires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import boto3\n",
    "import io\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score,\\\n",
    "roc_auc_score, make_scorer, plot_precision_recall_curve, plot_roc_curve, plot_confusion_matrix, average_precision_score,\\\n",
    "ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns',None)\n",
    "warnings.filterwarnings('ignore')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_s3(bucket,raw_data_path,raw_data_fnames):\n",
    "    s3 = boto3.client('s3')\n",
    "    dataset_nrows = []\n",
    "    for i, fname in enumerate(raw_data_fnames):\n",
    "        if i==0:\n",
    "            print(f'Reading file: {fname}')\n",
    "            key = f'{raw_data_path}/{fname}'\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            data = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "            dataset_nrows.append(data.shape[0])\n",
    "            print(f'\\tFile read successfully | Shape: {data.shape}')\n",
    "        else:\n",
    "            print(f'Reading file: {fname}')\n",
    "            key = f'{raw_data_path}/{fname}'\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            data2 = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "            data = data.append(data2,ignore_index=True)\n",
    "            dataset_nrows.append(data2.shape[0])\n",
    "            print(f'\\tFile read successfully | Shape: {data2.shape}')\n",
    "\n",
    "    if sum(dataset_nrows) == data.shape[0]:\n",
    "        print(f'Data from all files loaded successfully | Final Shape: {data.shape}')\n",
    "        return data.copy()\n",
    "    else:\n",
    "        print('There is discrepency in numbers')\n",
    "        print(f'\\tTotal number of rows combined in all files: {sum(dataset_nrows)}')\n",
    "        print(f'\\tAfter combining all files total number of rows are: {data.shape[0]}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: train_raw_data.csv\n"
     ]
    },
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2daed960861c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_data_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ffd670682894>\u001b[0m in \u001b[0;36mload_data_from_s3\u001b[0;34m(bucket, raw_data_path, raw_data_fnames)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reading file: {fname}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{raw_data_path}/{fname}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdataset_nrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 )\n\u001b[1;32m    514\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "train_set = load_data_from_s3(bucket,raw_data_path,raw_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of the Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Unnecessary columns\n",
    "drop_list = ['new_id','new_id_3','udp_global_id', 'state','mapped_employment_status_code', 'mapped_fullpart_description', \n",
    "             'mapped_permanent_temporary_description', 'mapped_hourly_salary_description', 'mapped_flex_status_description', \n",
    "            'original_hire_date', 'rehire_date', 'termination_date', 'base_pay_regular_frequency_description',\n",
    "             'base_pay_regular_expectedannualsalary_range','annual_benefits_base_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.drop(columns = drop_list, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning: columns are having mutiple entries for a same text, hence reducing them to individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Divorce_list = ['Divorced', 'Divorced_United_States_of_America', 'Separated_United_States_of_America', \\\n",
    "                           'Divorced_USA', 'USA_Divorced', 'USA_Separated', 'Separated_USA', 'Separated USA', 'Divorced USA', \\\n",
    "                           'Legally_Separated_United_States_of_America', 'USA-Divorced', 'Widowed_United_States_of_America', 'USA-Separated','Widowed_USA','PR_Divorced']\n",
    "Single_list = ['S', 'Single', 'Single_United_States_of_America', 'Single_USA', 'USA-Single', 'USA_Single', 'Single USA',\\\n",
    "                                           'S-USA', 'USA-Single, PR_Single', 'PR_Single']   \n",
    "Unknown_list = ['Unknown_USA', 'Unknown_United_States_of_America', 'Not_Indicated_United_States_of_America', \\\n",
    "                  'PR_Living Together','USA_Living together','USA_Not Disclosed', 'USA_Not Disclosed','Living_Together_United_States_of_America',\n",
    "       'Domestic_Partner _United_States_America', 'Civil P_United_States_of_America']\n",
    "Married_list = ['M','Married_United_States_of_America','Married_USA','Married USA','Married','M_USA','M-USA','USA-Married','USA_Married','Married_USA','Maried','USA-Married/ Civil Partnership','Domestic_Partner_United_States_of_America','Partnered_United_States_of_America','Co-Habiting_United_States_of_America','Partnered USA','D-USA']\n",
    "values = ['Single_Canada','Single_MEX', 'Single_DEU','Head_of_Household_USA','DE_FACTO','O-USA','MI_NOT_DISCLOSED','Domestic Partner', 'MARITAL_STATUS-3-321', 'Single_COL', 'PR_Partnered','Divorced_United_Kingdom','Single_MEX','MARITAL_STATUS-6-321','Unknown_United_States_of_America','Not_Indicated_United_States_of_America','Not_Disclosed_United_States_of_America','IN_Single',\n",
    "          'Unknown_USA','MARITAL_STATUS-3-321','Married_MEX', 'S-HKG',\n",
    "       'Unknown_New_Zealand', 'Unknown_Korea_Republic_of',\n",
    "       'Married_Saudi_Arabia', 'Single_United Kingdom', 'Single_Thailand',\n",
    "       'Unknown_United Kingdom', 'Unknown_India', 'Married_Taiwan',\n",
    "       'Single_Taiwan', 'Single_Hong_Kong', 'Single_Korea_Republic_of', 'P-USA', 'Married_BRA', 'Married_DEU',\n",
    "       'Unknown_SWE','Single_DOM',\n",
    "       'Married_CHN', 'Married_ESP', 'Single_THA',\n",
    "       'Married_Kenya', 'Unknown_Taiwan', 'Married_China',\n",
    "       'Married_New_Zealand', 'Single0_Indonesia', 'Single_Spain',\n",
    "       'Unknown_Thailand', 'Unknown_Spain', 'Married_Malaysia',\n",
    "       'Married2_Indonesia', 'Married_Thailand','MARITAL_STATUS-3-40', 'Married_United_Kingdom',\n",
    "       'Unknown_Puerto Rico','Single_United Arab Emirates', 'Divorced_Hong_Kong',\n",
    "       'Civil_Partnership_MEX', 'Married_SGP', 'Unknown_AUS',\n",
    "        'Single_SGP', 'Married_Spain',\n",
    "       'Divorced_Singapore', 'Divorced_Canada', 'Married_Italy',\n",
    "       'Married_MEX', 'Married_AUS', 'Single_ESP', 'Unknown_JPN',\n",
    "       'PR_Married', 'M-GBR', 'Married_Switzerland', 'Domestic Partner',\n",
    "       'W-USA', 'DE_FACTO', 'PR_Widowed', 'MI_NOT_DISCLOSED',\n",
    "       'Married_CAN', 'Divorced_DEU', 'Dissolved_Civil_Partnership_MEX',\n",
    "       'Single_FRA', 'Single_COL', 'Single_Puerto Rico', 'MARITAL_STATUS-6-301', 'MARITAL_STATUS-6-322', 'Hd Hsehld_United_States_of_America',\n",
    "       'Common_Law_United_States_of_America', 'Married_Ireland',\n",
    "       'Common_law_Canada', 'Married_Canada','Domestic_Partner_Canada',\n",
    "       'U-USA','MARITAL_STATUS-3-323', 'USA-Cohabit', 'Married0_Indonesia', 'Married_Korea_Republic_of', 'Single_China',\n",
    "       'Married1_Indonesia', 'Single_CAN', 'Married_SWE',\n",
    "       'Single_Belgium', 'Married_United Kingdom', 'Married_Puerto Rico',\n",
    "       'Married_ITA', 'Single_BEL', 'Married_BEL', 'Single_Lebanon',\n",
    "       'Unknown_Kenya', 'M-DEU', 'Unknown_GBR', 'Civil Partner_Belgium',\n",
    "       'Married_IRL', 'USA-Civil Partnership', 'Married_Hong_Kong',\n",
    "       'Domestic Partner Civil Union_United Kingdom',\n",
    "       'Civil_Partnership_COL', 'Unknown_Canada', 'Civil_Partnership_USA',\n",
    "       'Single_JPN', 'Married_MYS', 'Single_IRL', 'Single_New_Zealand',\n",
    "       'Married_Belgium', 'Civil_Partnership_United_States_of_America',\n",
    "       'Married_COL', 'Common_Law_COL', 'Married_Austria', 'Single_GBR',\n",
    "       'Married_GBR', 'Single_SWE', 'Married_JPN', 'Unknown_MLT',\n",
    "       'Married_NZL', 'Unknown_CHN', 'PR_Not Disclosed', 'R-USA', 'RDP',\n",
    "       'S-GBR', 'Married_Puerto_Rico', 'Married_Singapore', 'S-CHN',\n",
    "       'Married_United Arab Emirates', 'M-SGP', 'Single_Singapore',\n",
    "       'Separated_MLT', 'USA_Common-law', 'C-USA', 'S-ARE', 'S-SGP',\n",
    "       'Single_CHN', 'Single_United_Kingdom', 'M-CAN', 'M-IND',\n",
    "       'Single_BRA', 'Divorced_ESP', 'M-AUS', 'USA-Widowed',\n",
    "       'PRI-Married', 'Single_Argentina', 'Unspecified_United_Kingdom',\n",
    "       'Domestic_Partnered_United_States_of_America',\n",
    "       'Co-Habiting_United_Kingdom', 'Married_Turkey',\n",
    "       'Registered_Partnership_United_States_of_America',\n",
    "       'Unknown_Malaysia', 'USA-Unknown', 'USA_Widowed', 'M-HKG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.loc[train_set['marital_status'].isin(Divorce_list), 'marital_status'] = 'Divorced'\n",
    "train_set.loc[train_set['marital_status'].isin(Single_list), 'marital_status'] = 'Single'\n",
    "train_set.loc[train_set['marital_status'].isin(Married_list), 'marital_status'] = 'Married'\n",
    "train_set.loc[train_set['marital_status'].isin(Unknown_list), 'marital_status'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set[train_set.marital_status.isin(values) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['country_description'] = train_set['country_description'].replace(['United States of America', 'United States', 'UNITED STATES', 'USA'],'United States of America')\n",
    "train_set = train_set[train_set['country_description'] == 'United States of America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_fullpart_code'] = train_set['mapped_fullpart_code'].replace(['FT','PT'],['FLTM', 'PRTM'])\n",
    "fullpart_code_remove_list = ['DNM', 'ERROR']\n",
    "train_set = train_set[train_set.mapped_fullpart_code.isin(fullpart_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_permanent_temporary_code'] = train_set['mapped_permanent_temporary_code'].replace(['P','T'],['PERM', 'TEMP'])\n",
    "pertemp_code_remove_list = ['DNM', 'R', 'F', 'FR']\n",
    "train_set= train_set[train_set['mapped_permanent_temporary_code'].isin(pertemp_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_hourly_salary_code'] = train_set['mapped_hourly_salary_code'].replace(['S', 'H'],['SLRY', 'HRLY'])\n",
    "hr_code_remove_list = ['DNM', 'A', 'E', '1', 'L', 'N']\n",
    "train_set = train_set[train_set['mapped_hourly_salary_code'].isin(hr_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_flex_status_code'] = train_set['mapped_flex_status_code'].replace(['INELIG'],['NOTELIGIBLE'])\n",
    "flex_code_remove_list = ['DNM', 'ACTIVE', 'INACTIVE', 'TSDACT', 'RETIRE', 'HMFT']\n",
    "train_set = train_set[train_set['mapped_flex_status_code'].isin(flex_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = ['M', 'Male', 'male', 'Gender_Male']\n",
    "female = ['F', 'Female', 'female', 'Gender_Female']\n",
    "unknown = ['U', 'Unknown', 'Not_declared', 'D', 'Decline to answer', 'Undeclared',\n",
    "           'Not specified', 'Declined to State', 'N', 'O', 'Not Declared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.loc[train_set['gender'].isin(male), 'gender'] = 'male'\n",
    "train_set.loc[train_set['gender'].isin(female), 'gender'] = 'female'\n",
    "train_set.loc[train_set['gender'].isin(unknown), 'gender'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['gender'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['marital_status'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['is_union'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['platform_indicator_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_fullpart_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_hourly_salary_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['is_rehire'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['base_pay_regular_frequency_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_employment_status_description'] = train_set['mapped_employment_status_description'].replace([\"LOA - Workers Compensation\"], [\"LOA - Worker's Compensation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_employment_status_description'] = train_set['mapped_employment_status_description'].replace([\"LOA - no Pay\"], [\"LOA - No Pay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['mapped_employment_status_description'] = train_set['mapped_employment_status_description'].replace([\"LOA - with Pay\"], [\"LOA - With Pay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set.groupby(['mapped_employment_status_description'])['age'].quantile([0.01, 0.99]).unstack()\n",
    "capping=x.reset_index()\n",
    "capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(capping, how='left', on='mapped_employment_status_description')\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.rename(columns={0.01:'lower_age_cap',0.99:'upper_age_cap'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[['age','lower_age_cap','upper_age_cap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['age'] = np.where(train_set['age'] > train_set['upper_age_cap'], train_set['upper_age_cap'], train_set['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['age'] = np.where(train_set['age'] < train_set['lower_age_cap'], train_set['lower_age_cap'], train_set['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['base_pay_regular_payrate_amount','lower_age_cap','upper_age_cap','country_description','mapped_flex_status_code',\n",
    "    'mapped_permanent_temporary_code']\n",
    "train_set.drop(columns=col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_columns = train_set.select_dtypes(include=['object']).columns\n",
    "train_num_columns = train_set.select_dtypes(include=['int']).columns\n",
    "train_float_columns = train_set.select_dtypes(include=['float']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cat_columns)\n",
    "print(train_num_columns)\n",
    "print(train_float_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Imputer for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNumericImputer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, impute_cols, using_cols, method='median'):\n",
    "        self.using_cols = using_cols\n",
    "        self.impute_cols = impute_cols\n",
    "        self.method = method\n",
    "        cols = self.using_cols + self.impute_cols\n",
    "        X_ = X[cols]\n",
    "        \n",
    "        if self.method == 'median':\n",
    "            self.fit_values = X_.groupby(using_cols)[impute_cols].median().reset_index()\n",
    "            impute_cols_rename_dict = dict([(x,x+'_median') for x in self.impute_cols])\n",
    "            self.fit_values.rename(columns=impute_cols_rename_dict, inplace=True)\n",
    "            self.fit_values.fillna(self.fit_values.median(), inplace=True)\n",
    "        elif self.method == 'mean':\n",
    "            self.fit_values = X_.groupby(using_cols)[impute_cols].mean().reset_index()\n",
    "            impute_cols_rename_dict = dict([(x,x+'_mean') for x in self.impute_cols])\n",
    "            self.fit_values.rename(columns=impute_cols_rename_dict, inplace=True)\n",
    "            self.fit_values.fillna(self.fit_values.mean(), inplace=True)\n",
    "        else: print('Method can be \"median\" or \"mean\"')\n",
    "        self.new_cols = list(impute_cols_rename_dict.values())\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, impute_cols, using_cols, method='median'):\n",
    "        self.fit(X, impute_cols, using_cols, method)\n",
    "        \n",
    "        cols = self.using_cols + self.impute_cols\n",
    "        X_ = X[cols]\n",
    "        X_ = X_.merge(self.fit_values, how='left', on=self.using_cols)\n",
    "        for col, ncol in zip(self.impute_cols, self.new_cols):\n",
    "            X_[col] = np.where(X_[col].isnull(), X_[ncol], X_[col])\n",
    "            \n",
    "        if self.method == 'median':\n",
    "            return X_[self.impute_cols].fillna(X_[self.impute_cols].median())\n",
    "        elif self.method == 'mean':\n",
    "            return X_[self.impute_cols].fillna(X_[self.impute_cols].mean())\n",
    "    \n",
    "    def transform(self, X, impute_cols):\n",
    "        cols = self.using_cols + self.impute_cols\n",
    "        X_ = X[cols]\n",
    "        X_ = X_.merge(self.fit_values, how='left', on=self.using_cols)\n",
    "        for col, ncol in zip(self.impute_cols,self.new_cols):\n",
    "            X_[col] = np.where(X_[col].isnull(), X_[ncol], X_[col])\n",
    "            \n",
    "        if self.method == 'median':\n",
    "            return X_[self.impute_cols].fillna(X_[self.impute_cols].median())\n",
    "        elif self.method == 'mean':\n",
    "            return X_[self.impute_cols].fillna(X_[self.impute_cols].mean())\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_numeric_imputer = CustomNumericImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_num_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_float_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_float_columns] = custom_numeric_imputer.fit_transform(X=train_set, impute_cols=train_float_columns.tolist(),\n",
    "                                                              using_cols=['mapped_employment_status_description'], method='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_numeric_imputer.fit_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bins on the column base_pay_regular_expectedannualsalary and converting it into a range\n",
    "bins = [-1, 19999, 39999, 59999, 79999, 99999, 174999,249999, 999999999999]\n",
    "labels = ['<20,000', '20,000 - 39,999', '40,000 - 59,999', '60,000 - 79,999', '80,000 - 99,999', '100,000-174999', '175000-249999', '>250000']\n",
    "train_set['base_pay_regular_expected_annual_salary_range'] = pd.cut(x=train_set['base_pay_regular_expectedannualsalary'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop some columns which are not required\n",
    "col = ['person_internal_id','client_id','base_pay_regular_expectedannualsalary','mapped_employment_status_description']\n",
    "train_set.drop(columns=col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['age'] =train_set['age'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['base_pay_regular_expected_annual_salary_range'] =train_set['base_pay_regular_expected_annual_salary_range'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting pre-processed train data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uploading pre-processed data here -> s3://{bucket}/{pre_processed_train_data_path}')\n",
    "\n",
    "train_set.to_csv(f's3://{bucket}/{pre_processed_train_data_path}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_data_from_s3(bucket,raw_data_path,raw_data_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_employment_status_description'] = test_set['mapped_employment_status_description'].replace([\"LOA - Workers Compensation\"], [\"LOA - Worker's Compensation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_employment_status_description'] = test_set['mapped_employment_status_description'].replace([\"LOA - no Pay\"], [\"LOA - No Pay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_employment_status_description'] = test_set['mapped_employment_status_description'].replace([\"LOA - with Pay\"], [\"LOA - With Pay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Unnecessary columns\n",
    "drop_list = ['new_id','new_id_3','udp_global_id', 'state','mapped_employment_status_code', 'mapped_fullpart_description', \\\n",
    "             'mapped_permanent_temporary_description', 'mapped_hourly_salary_description', 'mapped_flex_status_description', \n",
    "            'original_hire_date', 'rehire_date', 'termination_date', 'base_pay_regular_frequency_description',\n",
    "             'base_pay_regular_expectedannualsalary_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.drop(columns = drop_list, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning: columns are having mutiple entries for a same text, hence reducing them to individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Divorce_list = ['Divorced', 'Divorced_United_States_of_America', 'Separated_United_States_of_America', \\\n",
    "                           'Divorced_USA', 'USA_Divorced', 'USA_Separated', 'Separated_USA', 'Separated USA', 'Divorced USA', \\\n",
    "                           'Legally_Separated_United_States_of_America', 'USA-Divorced', 'Widowed_United_States_of_America', 'USA-Separated','Widowed_USA','PR_Divorced']\n",
    "Single_list = ['S', 'Single', 'Single_United_States_of_America', 'Single_USA', 'USA-Single', 'USA_Single', 'Single USA',\\\n",
    "                                           'S-USA', 'USA-Single, PR_Single', 'PR_Single']   \n",
    "Unknown_list = ['Unknown_USA', 'Unknown_United_States_of_America', 'Not_Indicated_United_States_of_America', \\\n",
    "                  'PR_Living Together','USA_Living together','USA_Not Disclosed', 'USA_Not Disclosed','Living_Together_United_States_of_America',\n",
    "       'Domestic_Partner _United_States_America', 'Civil P_United_States_of_America']\n",
    "Married_list = ['M','Married_United_States_of_America','Married_USA','Married USA','Married','M_USA','M-USA','USA-Married','USA_Married','Married_USA','Maried','USA-Married/ Civil Partnership','Domestic_Partner_United_States_of_America','Partnered_United_States_of_America','Co-Habiting_United_States_of_America','Partnered USA','D-USA']\n",
    "values = ['Single_Canada','Single_MEX', 'Single_DEU','Head_of_Household_USA','DE_FACTO','O-USA','MI_NOT_DISCLOSED','Domestic Partner', 'MARITAL_STATUS-3-321', 'Single_COL', 'PR_Partnered','Divorced_United_Kingdom','Single_MEX','MARITAL_STATUS-6-321','Unknown_United_States_of_America','Not_Indicated_United_States_of_America','Not_Disclosed_United_States_of_America','IN_Single',\n",
    "          'Unknown_USA','MARITAL_STATUS-3-321','Married_MEX', 'S-HKG',\n",
    "       'Unknown_New_Zealand', 'Unknown_Korea_Republic_of',\n",
    "       'Married_Saudi_Arabia', 'Single_United Kingdom', 'Single_Thailand',\n",
    "       'Unknown_United Kingdom', 'Unknown_India', 'Married_Taiwan',\n",
    "       'Single_Taiwan', 'Single_Hong_Kong', 'Single_Korea_Republic_of', 'P-USA', 'Married_BRA', 'Married_DEU',\n",
    "       'Unknown_SWE','Single_DOM',\n",
    "       'Married_CHN', 'Married_ESP', 'Single_THA',\n",
    "       'Married_Kenya', 'Unknown_Taiwan', 'Married_China',\n",
    "       'Married_New_Zealand', 'Single0_Indonesia', 'Single_Spain',\n",
    "       'Unknown_Thailand', 'Unknown_Spain', 'Married_Malaysia',\n",
    "       'Married2_Indonesia', 'Married_Thailand','MARITAL_STATUS-3-40', 'Married_United_Kingdom',\n",
    "       'Unknown_Puerto Rico','Single_United Arab Emirates', 'Divorced_Hong_Kong',\n",
    "       'Civil_Partnership_MEX', 'Married_SGP', 'Unknown_AUS',\n",
    "        'Single_SGP', 'Married_Spain',\n",
    "       'Divorced_Singapore', 'Divorced_Canada', 'Married_Italy',\n",
    "       'Married_MEX', 'Married_AUS', 'Single_ESP', 'Unknown_JPN',\n",
    "       'PR_Married', 'M-GBR', 'Married_Switzerland', 'Domestic Partner',\n",
    "       'W-USA', 'DE_FACTO', 'PR_Widowed', 'MI_NOT_DISCLOSED',\n",
    "       'Married_CAN', 'Divorced_DEU', 'Dissolved_Civil_Partnership_MEX',\n",
    "       'Single_FRA', 'Single_COL', 'Single_Puerto Rico', 'MARITAL_STATUS-6-301', 'MARITAL_STATUS-6-322', 'Hd Hsehld_United_States_of_America',\n",
    "       'Common_Law_United_States_of_America', 'Married_Ireland',\n",
    "       'Common_law_Canada', 'Married_Canada','Domestic_Partner_Canada',\n",
    "       'U-USA','MARITAL_STATUS-3-323', 'USA-Cohabit', 'Married0_Indonesia', 'Married_Korea_Republic_of', 'Single_China',\n",
    "       'Married1_Indonesia', 'Single_CAN', 'Married_SWE',\n",
    "       'Single_Belgium', 'Married_United Kingdom', 'Married_Puerto Rico',\n",
    "       'Married_ITA', 'Single_BEL', 'Married_BEL', 'Single_Lebanon',\n",
    "       'Unknown_Kenya', 'M-DEU', 'Unknown_GBR', 'Civil Partner_Belgium',\n",
    "       'Married_IRL', 'USA-Civil Partnership', 'Married_Hong_Kong',\n",
    "       'Domestic Partner Civil Union_United Kingdom',\n",
    "       'Civil_Partnership_COL', 'Unknown_Canada', 'Civil_Partnership_USA',\n",
    "       'Single_JPN', 'Married_MYS', 'Single_IRL', 'Single_New_Zealand',\n",
    "       'Married_Belgium', 'Civil_Partnership_United_States_of_America',\n",
    "       'Married_COL', 'Common_Law_COL', 'Married_Austria', 'Single_GBR',\n",
    "       'Married_GBR', 'Single_SWE', 'Married_JPN', 'Unknown_MLT',\n",
    "       'Married_NZL', 'Unknown_CHN', 'PR_Not Disclosed', 'R-USA', 'RDP',\n",
    "       'S-GBR', 'Married_Puerto_Rico', 'Married_Singapore', 'S-CHN',\n",
    "       'Married_United Arab Emirates', 'M-SGP', 'Single_Singapore',\n",
    "       'Separated_MLT', 'USA_Common-law', 'C-USA', 'S-ARE', 'S-SGP',\n",
    "       'Single_CHN', 'Single_United_Kingdom', 'M-CAN', 'M-IND',\n",
    "       'Single_BRA', 'Divorced_ESP', 'M-AUS', 'USA-Widowed',\n",
    "       'PRI-Married', 'Single_Argentina', 'Unspecified_United_Kingdom',\n",
    "       'Domestic_Partnered_United_States_of_America',\n",
    "       'Co-Habiting_United_Kingdom', 'Married_Turkey',\n",
    "       'Registered_Partnership_United_States_of_America',\n",
    "       'Unknown_Malaysia', 'USA-Unknown', 'USA_Widowed', 'M-HKG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.loc[test_set['marital_status'].isin(Divorce_list), 'marital_status'] = 'Divorced'\n",
    "test_set.loc[test_set['marital_status'].isin(Single_list), 'marital_status'] = 'Single'\n",
    "test_set.loc[test_set['marital_status'].isin(Married_list), 'marital_status'] = 'Married'\n",
    "test_set.loc[test_set['marital_status'].isin(Unknown_list), 'marital_status'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set[test_set.marital_status.isin(values) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['country_description'] = test_set['country_description'].replace(['United States of America', 'United States', 'UNITED STATES', 'USA'],'United States of America')\n",
    "test_set = test_set[test_set['country_description'] == 'United States of America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_fullpart_code'] = test_set['mapped_fullpart_code'].replace(['FT','PT'],['FLTM', 'PRTM'])\n",
    "fullpart_code_remove_list = ['DNM', 'ERROR']\n",
    "test_set = test_set[test_set.mapped_fullpart_code.isin(fullpart_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_permanent_temporary_code'] = test_set['mapped_permanent_temporary_code'].replace(['P','T'],['PERM', 'TEMP'])\n",
    "pertemp_code_remove_list = ['DNM', 'R', 'F', 'FR']\n",
    "test_set= test_set[test_set['mapped_permanent_temporary_code'].isin(pertemp_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_hourly_salary_code'] = test_set['mapped_hourly_salary_code'].replace(['S', 'H'],['SLRY', 'HRLY'])\n",
    "hr_code_remove_list = ['DNM', 'A', 'E', '1', 'L', 'N']\n",
    "test_set = test_set[test_set['mapped_hourly_salary_code'].isin(hr_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_flex_status_code'] = test_set['mapped_flex_status_code'].replace(['INELIG'],['NOTELIGIBLE'])\n",
    "flex_code_remove_list = ['DNM', 'ACTIVE', 'INACTIVE', 'TSDACT', 'RETIRE', 'HMFT']\n",
    "test_set = test_set[test_set['mapped_flex_status_code'].isin(flex_code_remove_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = ['M', 'Male', 'male', 'Gender_Male']\n",
    "female = ['F', 'Female', 'female', 'Gender_Female']\n",
    "unknown = ['U', 'Unknown', 'Not_declared', 'D', 'Decline to answer', 'Undeclared',\n",
    "           'Not specified', 'Declined to State', 'N', 'O', 'Not Declared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.loc[test_set['gender'].isin(male), 'gender'] = 'male'\n",
    "test_set.loc[test_set['gender'].isin(female), 'gender'] = 'female'\n",
    "test_set.loc[test_set['gender'].isin(unknown), 'gender'] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set transformation (cleaning steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['gender'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['marital_status'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['is_union'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['platform_indicator_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_fullpart_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['mapped_hourly_salary_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['is_rehire'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['base_pay_regular_frequency_code'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.merge(capping, how='left', on='mapped_employment_status_description')\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.rename(columns={0.01:'lower_age_cap',0.99:'upper_age_cap'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['age'] = np.where(test_set['age'] > test_set['upper_age_cap'], test_set['upper_age_cap'], test_set['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['age'] = np.where(test_set['age'] < test_set['lower_age_cap'], test_set['lower_age_cap'], test_set['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['base_pay_regular_payrate_amount','lower_age_cap','upper_age_cap','country_description','mapped_flex_status_code',\n",
    "    'mapped_permanent_temporary_code']\n",
    "test_set.drop(columns=col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_columns = test_set.select_dtypes(include=['object']).columns\n",
    "test_num_columns = test_set.select_dtypes(include=['int']).columns\n",
    "test_float_columns = test_set.select_dtypes(include=['float']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_cat_columns)\n",
    "print(test_num_columns)\n",
    "print(test_float_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[test_float_columns] = custom_numeric_imputer.transform(X=test_set, impute_cols=test_float_columns)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[test_float_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bins on the column base_pay_regular_expectedannualsalary and converting it into a range\n",
    "bins = [-1, 19999, 39999, 59999, 79999, 99999, 174999,249999, 999999999999]\n",
    "labels = ['<20,000', '20,000 - 39,999', '40,000 - 59,999', '60,000 - 79,999', '80,000 - 99,999', '100,000-174999', '175000-249999', '>250000']\n",
    "test_set['base_pay_regular_expected_annual_salary_range'] = pd.cut(x=test_set['base_pay_regular_expectedannualsalary'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['age'] = test_set['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['base_pay_regular_expected_annual_salary_range'] = test_set['base_pay_regular_expected_annual_salary_range'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['person_internal_id','client_id','base_pay_regular_expectedannualsalary','mapped_employment_status_description']\n",
    "new_test_set=test_set.drop(columns=col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting pre-processed test data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uploading pre-processed data here -> s3://{bucket}/{pre_processed_test_data_path}')\n",
    "\n",
    "new_test_set.to_csv(f's3://{bucket}/{pre_processed_test_data_path}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot encoding the categorical variables in training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders --index-url https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple --trusted-host=artifactory.alight.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating out dependent and independent variables from the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(columns=['mapped_employment_status_code_label'], axis=1)\n",
    "y_train = train_set['mapped_employment_status_code_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = X_train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(cols=cat_columns, drop_invariant = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encoder.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot encoded data\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot encoding the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = encoder.transform(new_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --index-url https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple --trusted-host=artifactory.alight.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket, Key=model_path_with_fname)\n",
    "    fp.seek(0)\n",
    "    model = joblib.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'person_internal_id':test_set['person_internal_id'],'prediction':pred})\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uploading prediction results here -> s3://{bucket}/{prediction_output_path_with_fname}')\n",
    "\n",
    "result.to_csv(f's3://{bucket}/{prediction_output_path_with_fname}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Raw Data is available at ->\\n\\ts3://{bucket}/{raw_data_path}/{raw_data_train}')\n",
    "print(f'\\nPre-processed train data is available at ->\\n\\ts3://{bucket}/{pre_processed_train_data_path}')\n",
    "print(f'\\nPre-processed data is available at ->\\n\\ts3://{bucket}/{pre_processed_test_data_path}')\n",
    "print(f'\\nPrediction results is available at ->\\n\\ts3://{bucket}/{prediction_output_path_with_fname}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
